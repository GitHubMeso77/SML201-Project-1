---
title:  'SML 201 Project 4: Forecasting Credit Card Balance'
subtitle: ''
author: 'Mesonma Anwasi and Amber Rahman'
date: 'Due Date: May 3, 2022'
output:
  pdf_document:
    dev: png
    fig_caption: yes
    toc: no
    toc_depth: 3
  html_document:
    df_print: paged
    toc: no
    toc_depth: 3
geometry: margin=1in
fontsize: 10pt
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align = "center",
                      fig.height = 6,
                      fig.width = 8,
                      collapse = T,
                      comment = "",
                      prompt=F, 
                      echo = T, 
                      cache = T,
                      #cache.extra = rand_seed,
                      autodep = T, 
                      tidy = T, 
                      tidy.opts=list(width.cutoff=63),
                      dev = 'png')
options(width=63)

```


**Project 4 is due by 05:00pm EST on Tuesday May 3 (Dean's Date).** Please submit both a .Rmd and a .pdf file on Gradescope by the deadline.

**No late projects** will be accepted after the deadline. Princeton University forbids instructors granting extensions past Deanâ€™s Date without the approval of students college Dean or Director of Study.

**Please do not email your work to any of the instructors in any case** (unless you have no access to Gradescope at the time assignment is due--in this case please email the grader both copies of your .pdf and .rmd files before the deadline and upload the unaltered copies of the files to Gradescope as soon as you regain access to Gradescope).  We need your files to be on Gradescope in order to grade them.  Thus, even if you are late, please still submit the files on Gradescope.

This project can be completed in groups of up to 3 students. Feel free to use Ed Discussion *Search for Teammates* post to recruit team members.  It is okay to work by yourself, if this is preferable.   You are welcome to get help (you can either ask questions on Ed Discussion or talk to instructors in person during office hours) from instructors; however, please do not post code/solutions on Ed Discussion on a public post. 
For **projects** you may not work with a given student on more than two project. For example, if you work with student A on Projects 1 and 2, then you cannot work with student A on any other projects. 

When working in a group it is your responsibility to make sure that you are satisfied with all parts of the report and the submission is on time (e.g., we will not entertain arguments that deficiencies are the responsibility of other group members).  We expect that you each work independently first and then compare your answers with each other once you all finish, or help each other if someone in your group gets stuck.  Failing to make contributions and then putting your name on a report will be considered a violation of the honor code.  **Please do not divide work among group mates.**  Everyone in your group is responsible for being able to explain the solutions in the report.

For all parts of this project, you **MUST** use R commands to print the output as part of your R Markdown file. You are not permitted to find the answer in the R console and then copy and paste the answer into this document. Except stated otherwise, all plots must be done using the `ggplot2` package. They should be properly labeled with title and legend if applicable. It is your responsibility to make them clearly visible with no further processing.

**If you are completing this project in a group**, please have only **one** person in your group turn in the .Rmd and .pdf files; if you are not sure about how to make a group submission please see the video posted under the *Precepts* on Canvas.


----

Please type your name(s) after "Digitally signed:" below the honor pledge to serve as digital signature(s).  Put the pledge and your signature(s) at the beginning of each document that you turn in.

> I pledge my honor that I have not violated the honor code when completing this assignment.

Digitally signed: Mesonma Anwasi and Amber Rahman

----

# Objective of this project

(Hypothetical)  Congrats!  You have been hired as an intern at Capital One (https://www.capitalone.com/) in Portland, Oregon.  As for your first project your manager would like to build a new model to predict the average monthly Credit Card balance for new clients in your area.

Later on, your manager is interested in using your model as an input to the Risk department so that your predictions can be used to calculate default risk.

# Background info and the dataset

You have a dataset called `clients` to build the model. The dataset contains 400 anonymous clients with 14 variables including socio-demographic characteristics and financial information data collected between May 2014 and May 2015.

The definitions for the variables in the dataset are listed below. 

* **Income**: Annual income
* **Limit**: Credit Card Monthly Limit
* **Rating**: Credit Rating 
* **Cards**: The number of credit cards
* **Age**: Age in years
* **Education** : Number of years of education
* **Gender**: Male and Female
* **Student**: Is the client Student?
* **Married**: Is the client Married?
* **Ethnicity**: African American, Asian or Caucasian
* **Balance**: Average credit card balance in USD
* **Children**: Number of Children
* **Debt**: The total client's debt
* **Asset**: The total client's asset


## R Packages

You will need the following packages for this project
```{r, results='hide'}
library(readr)
library(tidymodels)
library(GGally)
library(coefplot)
```

# Load and Tidy-up Data

## Question 1 (3 pts)

### Part (a) (1 pt)

Read the dataset `client.csv` into `R` and name it `client`. Verify that `client` have 4000 rows and 14 columns.  

```{r}
client = read_csv('/Users/meso_alexis/SML201/SML201_2022Spring/data/client.csv')
nrow(client)
ncol(client)
```

### Part (b) (2 pts)

Convert the `character`-type variables to `factor` (categorical variables)

```{r}
client$Gender<-as.factor(client$Gender)
client$Student<-as.factor(client$Student)
client$Married<-as.factor(client$Married)
client$Ethnicity<-as.factor(client$Ethnicity)
client$Children<-as.factor(client$Children)
client
```


# Hypothesis Testing 

## Question 2 (17 pts)

### Part (a) (5 pts)

Test the null hypothesis that the mean `Income` for the different `Ethnicity` is the same at $1\%$ significance level. Report the p-value of the test.

```{r}
anovaie = aov(client$Income~client$Ethnicity)
summary(anovaie)
```

P-Value: 0.000434
Here we reject the null hypothesis because the p-value is less than 0.01

### Part (b) (6 pts)

Test the null hypothesis that married people have greater `Rating` than non-married ones at $5\%$ significance level. Report the p-value of the test.

```{r}
anovarm = aov(client$Rating~client$Married)
summary(anovarm)
```

P-value: 0.0655
Here we do not reject the null hypothesis because the p-value is greater than 0.05

### Part (c) (6 pts)

Test the null hypothesis that the mean of the ratio `Limit` to `Card`, i.e., the average limit per credit card,  is same for male and female at $10\%$ significance level. Report the p-value of the test.

```{r}
anovalcg = aov(client$Limit/client$Cards~client$Gender)
summary(anovalcg)
```

P-value: 0.249
Here we do not reject the null hypothesis because the p-value is greater than 0.1

# Exploring `Balance`  and its relationship between the other variables in the data set

## Question 3 (11 pts)

### Part (a) (3 pts)

Is the distribution of the variable `Balance` symmetric or skewed? If it is skewed, does it have a long left or right tail?  Please provide one graph to support your statement.

```{r}
qqnorm(client$Balance)
#plot qqplot
```

The graph is skewed, so it has a long right tail.

### Part (b) (4 pts)

If you were to choose one *categorical* variable to predict `Balance` which one would you choose? Plot a graph to support your answer

```{r}
#violin plot shows the variation.
ggplot(client, aes(x=Balance, y=Student)) + geom_violin()
ggplot(client, aes(x=Balance, y=Gender)) + geom_violin()
ggplot(client, aes(x=Balance, y=Married)) + geom_violin()
ggplot(client, aes(x=Balance, y=Ethnicity)) + geom_violin()
ggplot(client, aes(x=Balance, y=Children)) + geom_violin()
```

We chose the categorical variable 'Student' to predict 'Balance' because its 
violin plot shows greater variation between 'Student_Yes' and 'Student_No' than 
the other violin plots.

### Part (c) (4 pts)

If you were to choose one *continuous* variable to predict `Balance` which one would you choose. Plot a graph to support your answer

```{r}
ggplot(client, aes(Balance, Limit)) + geom_point()
```

We would choose 'Limit' because it has the a strong, positive correlation with 'Balance'.

# Model building

Let's build our model! But first we need to split the data...


## Question 4: Split the Data (5 pts)

Set seed 1 and divide the data into

* `model_fit_data` (90% observations of `client`): 
* `test2` 

Then, further split `model_fit_data` into

* `non_test` (85% observations of `model_fit_data`):
* `test`

**NOTE:** In both splits you should keep the proportion of the categorical variable chosen in Question (b) roughly the same.

```{r}
set.seed(1)
splitclient = initial_split(client, strata='Student', prop=0.9)
model_fit_data = training(splitclient)
test2 = testing(splitclient)

splitclient2=initial_split(model_fit_data, strata='Student', prop=0.85)
non_test=training(splitclient2)
test=testing(splitclient2)
```


## Question 5: OLS Model (12 pts) 

Now  you will build a linear regression model to predict `Balance` using all the other variables available in `client`

### Part (a) (5 pts) 

Fit by OLS model using the dataset `non_test`. Name the output of your fitted OLS model `ols.mod`. Report the estimated coefficients.

```{r}
ols.mod <- lm(formula= Balance ~ Income + Limit + Rating 
              + Cards + Age + Education + Asset + Debt, data=non_test)
ols.mod
```

Coefficients: 
Income: -5.590e+00, Limit: 1.748e-01, Rating: 9.882e-01, Cards: 1.419e+01, 
Age: -7.797e-01, Education: 4.351e-03, Asset: 1.403e-03, Debt: -3.738e-04

Intercept: -3.529e+02

### Part (b) (7 pts) 

How many regressors we have in the model `ols.mod` (number of coefficient excluding the intercept)?

What is the $\text{RMSE}$ when you use `ols.mod` to predict `Balance` in the `non-test` dataset ?  What about in the `test` dataset?

```{r}
p<-fitted.values(ols.mod)
nontestrmse<-sqrt(mean((non_test$Balance - p)^2))
nontestrmse

testrmse<-sqrt(mean((test$Balance - p)^2))
testrmse

```

We have 8 regressors.

'ols.mod' RMSE with non-test data: 177.4638
'ols.mod' RMSE with test data: 707.4694

## Question 6: LASSO Model (30 pts)

Here we will build a linear model with the LASSO.

### Part (a) (17 pts)

Build a LASSO regression model with all the variables available to us. Expand all the categorical variables into multiple dummies using `one_hot = FALSE`.

You will choose the penalty parameter `lambda` by 10-fold cross validations using the `non_test` dataset. For that use `lambda.grid` below as the grid for $\lambda$ values to be consider. Just run the code below

```{r}
lambda.max = 415
n.grid = 200
lambda.min.ratio = 1e-4
lambda.min = lambda.min.ratio*lambda.max
lambda.grid = exp(seq(log(lambda.min),
                      log(lambda.max),
                      length.out = n.grid))
lambda.grid = data.frame(penalty = lambda.grid)
```

You will need to use `lambda.grid` as the input for the argument `grid` of the function `tune_grid()` as in

```{r}
# model specification
set.seed(1)
client.recipe <- recipe(Balance~., data=non_test) %>% step_dummy(all_nominal(), one_hot=FALSE)

mod_spec<-linear_reg(penalty=tune()) %>% set_engine('glmnet')

flow.client = workflow() %>% 
    add_recipe(client.recipe) %>% 
    add_model(mod_spec) 

set.seed(1)
cv_client <- vfold_cv(data=non_test, v=10, strata='Student', repeats=5)

tune.result = tune_grid(object=flow.client, grid=lambda.grid, resamples=cv_client,
                        control=control_grid(verbose=TRUE))

```


Set seed 1 and repeat the 10-fold cross validation process 5 times. Also, make sure that all folds have about the same proportion of categorical variable used in Question 4.

Use $\text{RMSE}$ as the Loss function in the cross validation.


```{r}
# set.seed(1)
# cv_client <- vfold_cv(data=non_test, v=10, strata='Student', repeats=5)
loss_client = metric_set(rmse)
```

We decided to run the cross validation before the tune.result in order to use 
its value for the resampling of the tune.result.

### Part (b) (3 pts)

Plot the $\text{RMSE}$ v.s. $\lambda$ values. Find $\lambda_{\text{min}}$ that corresponds to value of $\lambda$ with the smallest $\text{RMSE}$?

```{r}
library(tictoc)
tune.autoplot <- tune.result %>% autoplot()
tune.autoplot

tune.result %>% show_best(metric='rmse', n = 1)
```
The lambda minimum value that corresponds to the value of lambda with the 
smallest RMSE is 0.732

### Part (c) (2 pts)

Select the $\lambda_{\text{champion}}$ that corresponds to the simplest model whose $\text{RMSE}$ is within one SE of the lowest $\text{RMSE}$. Report the value of the chosen $\lambda_{\text{champion}}$.

```{r}
lambda.champion = tune.result %>% select_by_one_std_err(metric='rmse', desc(penalty))

lambda.champion %>% pull(penalty)
```

Value of chosen lambda champion: 7.751841

### Part (d) (3 pts)

Using $\lambda_{\text{champion}}$ fit the champion model in the entire `non_test` dataset and report the estimates. 

```{r}
champion.flow=flow.client %>% finalize_workflow(parameters=lambda.champion)

champion.model = fit(champion.flow, data=non_test)

#get coefficient estimates
champion.model %>% extract_fit_engine() %>% coefplot(lambda=lambda.champion %>% pull(penalty), plot=FALSE)
```
Income: -5.3737595 , Limit: 0.1704534, Rating: 0.9438790 , Cards: 8.5943323, 
Age: -0.2900036, Student_Yes: -0.2900036

Intercept: -370.1165979

### Part (e) (2 pts)

How many predictors does your champion model have?  (Predictors are the X-variables in the mathematical form of your model, so the y-intercept is not a predictor).  Compare this number with that for the OLS model.

The champion model has 6 predictors whereas there are 8 in the OLS model.

### Part (f) (3 pts)

What is the $\text{RMSE}$ when you use champion model to predict `Balance` in the `non-test` dataset ?  What about in the `test` dataset? Compare this number with that for the OLS model.

```{r}
pred.non_test = predict(champion.model, new_data = non_test)

RMSE_non_test = sqrt(mean((non_test$Balance - pull(pred.non_test))^2))
RMSE_non_test

pred.test = predict(champion.model, new_data = test)

RMSE_test = sqrt(mean((test$Balance - pull(pred.test))^2))
RMSE_test

```

RMSE non_test: 136.9386
RMSE test: 139.9525

When we use champion.model to predict Balance with the non_test dataset, we get 
an RMSE value of 136.9386, and when we use the test dataset, we get an RMSE 
value of 139.9525, whereas in ols.mod, when predicting Balance with the non_test
dataset, we get an RMSE value of 177.4638, and when predicting Balance with the
test dataset, we get an RMSE value of 707.4694. This means the ols.mod has a 
higher RMSE than champion.model.

# Try to improve your model

## Question 7: Include interactions (13 pts)

### Part (a) (5 pts)

Include in your recipe the interaction terms between the numerical variables and the categorical variables included in the Champion model. Print the head of the pre-process data to confirm the correct inclusion of the interaction terms

```{r}
rec2 = recipe(formula = Balance ~ ., data = non_test) %>%
step_dummy(all_nominal(), one_hot=FALSE) %>%
step_interact(~ Income:starts_with("Student")) %>% 
step_interact(~ Rating:starts_with("Student")) %>% 
step_interact(~ Cards:starts_with("Student")) %>% 
step_interact(~ Limit:starts_with("Student")) %>% 
step_interact(~ Age:starts_with("Student"))
```


```{r}
print.data.frame(
  head(
    bake(
      prep(rec2), 
      new_data = non_test
    )
  )
)
```


### Part (b) (8 pts)

Refit your LASSO model with the recipe of part(a) following  steps of Question 6 (a)-(f)

```{r}
set.seed(1)
rec2

mod_spec<-linear_reg(penalty=tune()) %>% set_engine('glmnet')

flow.client2 = workflow() %>% 
    add_recipe(rec2) %>% 
    add_model(mod_spec) 

tune.result2 = tune_grid(object=flow.client2, grid=lambda.grid, resamples=cv_client,
                        control=control_grid(verbose=TRUE))

tune.autoplot2 <- tune.result2 %>% autoplot()
tune.autoplot2

lambda.champion2 = tune.result2 %>% select_by_one_std_err(metric='rmse', desc(penalty))
lambda.champion2 %>% pull(penalty)

champion.flow2=flow.client2 %>% finalize_workflow(parameters=lambda.champion2)
champion.flow2
champion.model2 = fit(champion.flow2, data=non_test)

#get coefficient estimates
champion.model2 %>% extract_fit_engine() %>% coefplot(lambda=lambda.champion2 
%>% pull(penalty), plot=FALSE)

# here and below gives error

pred.non_test2 = predict(champion.model2, new_data = non_test)

RMSE_non_test2 = sqrt(mean((non_test$Balance - pull(pred.non_test2))^2))
RMSE_non_test2

pred.test2 = predict(champion.model2, new_data = test)

RMSE_test2 = sqrt(mean((test$Balance - pull(pred.test2))^2))
RMSE_test2
```

# Your Final Decision

## Question 8 (9 pts)

### Part (a) (5 pts)

Based on what you have learned so far, which model you would choose as your final model to predict `Balance`? Justify your choice in light of your answers to Question 5, 6 and 7. In particular the trade-off between complexity of the model (number of predictors) and the forecasting error ($\text{RMSE}$)

Additionally, because the RMSE values for 'champion.model' (137.2833 for nontest 
and 139.8666 for test) are lower than the values for 'ols.mod' 
(177.4638 and 707.4694). Champion.model has a similar RMSE to 'champion.model2' 
(135.3774 for nontest and 138.5976 for test), so we will choose 'champion.model' 
to predict 'Balance' because it has less regressors, which makes it less complex.

### Part (b) (4 pts)

Using your recommendation in  (a), predict the `Balance` in `test2` dataset and report the $\text{RMSE}$ (to your manager)

```{r}
pred.test=predict(champion.model, new_data=test2)
RMSE_test=sqrt(mean((test$Balance - pull(pred.test))^2))
RMSE_test
```

RMSE value for test2 in champion.model is : 689.0175
